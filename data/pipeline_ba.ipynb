{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58abbac1-46a1-4a3c-8ecb-981ccaea6704",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine, text as sql_text\n",
    "from datetime import datetime, timedelta\n",
    "import openpyxl\n",
    "from scipy.stats import zscore\n",
    "from scipy.stats import norm\n",
    "from pandas.tseries.offsets import DateOffset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab4eefa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_dict = {\n",
    "    'umsatz': 'revenue',\n",
    "    'umsatz-1': 'revenue-1',\n",
    "    'umsatz-2': 'revenue-2',\n",
    "    'umsatz-3': 'revenue-3',\n",
    "    'umsatzVJ': 'revenuePY',\n",
    "    'umsatzVJ-1': 'revenuePY-1',\n",
    "    'umsatzVJ-2': 'revenuePY-2',\n",
    "    'umsatzVJ-3': 'revenuePY-3',\n",
    "    'umsatzVJ+2': 'revenuePY+2',\n",
    "    'umsatzVJ+1': 'revenuePY+1',\n",
    "    'umsatz+2': 'revenue+2',\n",
    "    'karneval': 'carnival',\n",
    "    'ostern': 'easter',\n",
    "    'himmelfahrt': 'ascension_day',\n",
    "    'pfingsten': 'whitsunday',\n",
    "    'weihnachten': 'christmas',\n",
    "    'neujahr': 'new_year'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7f129db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_previous_year_monday(row):\n",
    "    # Extrahiere Jahr und Kalenderwoche aus der 'kalenderwoche'-Spalte\n",
    "    week, year = map(int, row['kalenderwoche'].split('-'))\n",
    "    year -= 1  # Vorjahr\n",
    "\n",
    "    # Finde den ersten Montag des Vorjahres\n",
    "    first_day_of_year = datetime(year, 1, 1)\n",
    "\n",
    "    # Berechne den ersten Montag des Vorjahres\n",
    "    first_monday = first_day_of_year + timedelta(days=(7 - first_day_of_year.weekday())) if first_day_of_year.weekday() != 0 else first_day_of_year\n",
    "\n",
    "    # Berechne den Montag der entsprechenden Kalenderwoche des Vorjahres\n",
    "    previous_year_monday = first_monday + timedelta(weeks=week - 1)\n",
    "    return previous_year_monday\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5569e389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion zur Berechnung des Medcouple (MC)\n",
    "def medcouple(x):\n",
    "    x = np.sort(x)\n",
    "    n = len(x)\n",
    "    med = np.median(x)\n",
    "    below_med = x[x < med]\n",
    "    above_med = x[x > med]\n",
    "    \n",
    "    if len(below_med) == 0 or len(above_med) == 0:\n",
    "        return 0\n",
    "    \n",
    "    pairwise_diff = (above_med[:, None] - below_med[None, :])\n",
    "    medcouple_scores = ((above_med[:, None] - med) - (med - below_med[None, :])) / pairwise_diff\n",
    "    \n",
    "    return np.median(medcouple_scores[np.isfinite(medcouple_scores)])\n",
    "\n",
    "# Funktion zur Berechnung der robusten Skala (MAD)\n",
    "def robust_scale(y):\n",
    "    med = y.median()\n",
    "    mad = ((y - med).abs()).median() / norm.ppf(0.75)\n",
    "    return mad\n",
    "\n",
    "# Funktion zur Berechnung der Adjusted Outlyingness (AO) mit Pandas\n",
    "def adjusted_outlyingness(group, column):\n",
    "    y = group[column]\n",
    "    med = y.median()\n",
    "    mc = medcouple(y)\n",
    "    \n",
    "    # Berechnung der Quartile und des IQR\n",
    "    Q1 = y.quantile(0.25)\n",
    "    Q3 = y.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    # Anpassung der Whisker basierend auf MC\n",
    "    if mc > 0:\n",
    "        lower_whisker = Q1 - 1.5 * np.exp(-4 * mc) * IQR\n",
    "        upper_whisker = Q3 + 1.5 * np.exp(3 * mc) * IQR\n",
    "    else:\n",
    "        lower_whisker = Q1 - 1.5 * np.exp(-3 * mc) * IQR\n",
    "        upper_whisker = Q3 + 1.5 * np.exp(4 * mc) * IQR\n",
    "    \n",
    "    # Berechnung der AO-Werte\n",
    "    AO = pd.Series(index=y.index, dtype=float)\n",
    "    AO[y >= med] = (y[y >= med] - med) / (upper_whisker - med)\n",
    "    AO[y < med] = (med - y[y < med]) / (med - lower_whisker)\n",
    "    \n",
    "    # Identifizierung der Ausreißer basierend auf den angepassten Whiskern\n",
    "    \n",
    "    group['is_outlier'] = (y < lower_whisker) | (y > upper_whisker)\n",
    "    group['AO'] = AO\n",
    "    \n",
    "    return group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44fce7c",
   "metadata": {},
   "source": [
    "**Laden der Daten**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbaf456e",
   "metadata": {},
   "outputs": [],
   "source": [
    "datei_pfad = 'bakery_data.csv'\n",
    "df_roh = pd.read_csv(datei_pfad)\n",
    "df_roh = df_roh[df_roh['kd_plz'].notna()]\n",
    "df_roh = df_roh[df_roh['umsatz'].notna()]\n",
    "df_roh['kd_plz'] = df_roh['kd_plz'].astype(int)\n",
    "df_roh['datum'] = pd.to_datetime(df_roh['datum'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bcea8d",
   "metadata": {},
   "source": [
    "**Add Bundesland**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "203e5b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bd = pd.read_csv(\"../data/PLZ_BD_Mapping.csv\",sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ba6b06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bd = df_bd.rename({'PLZ' : 'kd_plz', 'BUNDESLAND' : 'Bundesland'}, axis = 1)\n",
    "df_bd['kd_plz'] = df_bd['kd_plz'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50f827f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bd = df_bd.groupby('kd_plz')['Bundesland'].agg(lambda x: x.mode().iloc[0]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99a88f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_roh= df_roh.merge(df_bd, on = 'kd_plz', how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac062654",
   "metadata": {},
   "source": [
    "**Add Vorjahr**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25c7cdd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_shift = df_roh.copy()\n",
    "df_no_shift['datumVJ'] = df_no_shift.apply(get_previous_year_monday, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "201d533b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kalender = pd.read_excel(\"kalender.xlsx\")\n",
    "kalender_nrw = pd.read_excel(\"kalender_NRW.xlsx\")\n",
    "kalender_nds = pd.read_excel(\"kalender_NDS.xlsx\")\n",
    "kalender_rp= pd.read_excel(\"kalender_RP.xlsx\")\n",
    "kalender_bawü = pd.read_excel(\"kalender_BaWü.xlsx\")\n",
    "kalender_he = pd.read_excel(\"kalender_HE.xlsx\")\n",
    "kalender_scho = pd.read_excel(\"kalender_ScHo.xlsx\")\n",
    "\n",
    "kalender = kalender[['datum', 'datumVJ']]\n",
    "kalender_nrw = kalender_nrw[['datum', 'datumVJ']]\n",
    "kalender_nds = kalender_nds[['datum', 'datumVJ']]\n",
    "kalender_rp = kalender_rp[['datum', 'datumVJ']]\n",
    "kalender_bawü = kalender_bawü[['datum', 'datumVJ']]\n",
    "kalender_he = kalender_he[['datum', 'datumVJ']]\n",
    "kalender_scho = kalender_scho[['datum', 'datumVJ']]\n",
    "\n",
    "kalender['datum'] = pd.to_datetime(kalender['datum'], format='%d.%m.%Y')\n",
    "kalender['datumVJ'] = pd.to_datetime(kalender['datumVJ'], format='%d.%m.%Y')\n",
    "\n",
    "kalender_nrw['datum'] = pd.to_datetime(kalender_nrw['datum'], format='%d.%m.%Y')\n",
    "kalender_nrw['datumVJ'] = pd.to_datetime(kalender_nrw['datumVJ'], format='%d.%m.%Y')\n",
    "\n",
    "kalender_nds['datum'] = pd.to_datetime(kalender_nds['datum'], format='%d.%m.%Y')\n",
    "kalender_nds['datumVJ'] = pd.to_datetime(kalender_nds['datumVJ'], format='%d.%m.%Y')\n",
    "\n",
    "kalender_he['datum'] = pd.to_datetime(kalender_he['datum'], format='%d.%m.%Y')\n",
    "kalender_he['datumVJ'] = pd.to_datetime(kalender_he['datumVJ'], format='%d.%m.%Y')\n",
    "\n",
    "kalender_rp['datum'] = pd.to_datetime(kalender_rp['datum'], format='%d.%m.%Y')\n",
    "kalender_rp['datumVJ'] = pd.to_datetime(kalender_rp['datumVJ'], format='%d.%m.%Y')\n",
    "\n",
    "kalender_bawü['datum'] = pd.to_datetime(kalender_bawü['datum'], format='%d.%m.%Y')\n",
    "kalender_bawü['datumVJ'] = pd.to_datetime(kalender_bawü['datumVJ'], format='%d.%m.%Y')\n",
    "\n",
    "kalender_scho['datum'] = pd.to_datetime(kalender_scho['datum'], format='%d.%m.%Y')\n",
    "kalender_scho['datumVJ'] = pd.to_datetime(kalender_scho['datumVJ'], format='%d.%m.%Y')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7d0bc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataframes = {\n",
    "    'Nordrhein-Westfalen': kalender_nrw,\n",
    "    'Hessen': kalender_he,\n",
    "    'Rheinland-Pfalz': kalender_rp,\n",
    "    'Niedersachsen': kalender_nds,\n",
    "    'Schleswig-Holstein': kalender_scho,\n",
    "    'Baden-Württemberg': kalender_bawü,\n",
    "    \n",
    "}\n",
    "merged_df_list = []\n",
    "\n",
    "# Iterieren Sie über die einzigartigen Bundesländer im Haupt-DataFrame\n",
    "for bundesland in df_roh['Bundesland'].unique():\n",
    "    if bundesland in dataframes:\n",
    "        # Filter für das aktuelle Bundesland\n",
    "        df_filtered = df_roh[df_roh['Bundesland'] == bundesland]\n",
    "        # Merge mit dem entsprechenden DataFrame\n",
    "        merged = pd.merge(df_filtered, dataframes[bundesland], on='datum', how='left')\n",
    "        # Hinzufügen zum Ergebnis-DataFrame\n",
    "        merged_df_list.append(merged)\n",
    "\n",
    "# Zusammenfügen aller Teil-DataFrames\n",
    "df_school_shift = pd.concat(merged_df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d163d813",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_public_shift = pd.merge(df_roh, kalender, on ='datum', how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef729e9",
   "metadata": {},
   "source": [
    "**Add umsatzVJ for Analyse**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e778c26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_shift = df_no_shift.merge(df_no_shift[['datum', 'filialnummer', 'Bäckerei', 'umsatz']], left_on=['datumVJ', 'filialnummer', 'Bäckerei'], right_on=['datum', 'filialnummer', 'Bäckerei'], how='left', suffixes=('', 'VJ'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "725ab1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_public_shift = df_public_shift.merge(df_public_shift[['datum', 'filialnummer', 'Bäckerei', 'umsatz']], left_on=['datumVJ', 'filialnummer', 'Bäckerei'], right_on=['datum', 'filialnummer', 'Bäckerei'], how='left', suffixes=('', 'VJ'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21e7a80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_school_shift = df_school_shift.merge(df_school_shift[['datum', 'filialnummer', 'Bäckerei', 'umsatz']], left_on=['datumVJ', 'filialnummer', 'Bäckerei'], right_on=['datum', 'filialnummer', 'Bäckerei'], how='left', suffixes=('', 'VJ'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "951c6bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_shift = df_no_shift.loc[:, ~df_no_shift.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "70f102f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_public_shift = df_public_shift.loc[:, ~df_public_shift.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81728072",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_school_shift = df_school_shift.loc[:, ~df_school_shift.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b32b4f4",
   "metadata": {},
   "source": [
    "**Safe as .csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f33a432f-52a0-4fd8-b2a9-f37bd883bfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_shift.to_csv('no_shift_roh.csv', index=False)\n",
    "df_no_shift = df_no_shift.drop(['umsatzVJ'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f5b091a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_public_shift.to_csv('public_shift_roh.csv', index=False)\n",
    "df_public_shift = df_public_shift.drop(['umsatzVJ'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9167b7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_school_shift.to_csv('school_shift_roh.csv', index=False)\n",
    "df_school_shift = df_school_shift.drop(['umsatzVJ'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2e87a4",
   "metadata": {},
   "source": [
    "**Outlier (AO)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "20d2164c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aaron\\AppData\\Local\\Temp\\ipykernel_3384\\175811738.py:1: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_no_shift = df_no_shift.groupby(['datum'], group_keys=False).apply(adjusted_outlyingness, column = 'umsatz').reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "df_no_shift = df_no_shift.groupby(['datum'], group_keys=False).apply(adjusted_outlyingness, column = 'umsatz').reset_index(drop=True)\n",
    "df_no_shift = df_no_shift[df_no_shift['is_outlier'] == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "394e0a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aaron\\AppData\\Local\\Temp\\ipykernel_3384\\3390694159.py:1: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_public_shift = df_public_shift.groupby(['datum'], group_keys=False).apply(adjusted_outlyingness, column = 'umsatz').reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "df_public_shift = df_public_shift.groupby(['datum'], group_keys=False).apply(adjusted_outlyingness, column = 'umsatz').reset_index(drop=True)\n",
    "df_public_shift = df_public_shift[df_public_shift['is_outlier'] == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3040b96d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aaron\\AppData\\Local\\Temp\\ipykernel_3384\\1532561050.py:1: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_school_shift = df_school_shift.groupby(['datum'], group_keys=False).apply(adjusted_outlyingness, column = 'umsatz').reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "df_school_shift = df_school_shift.groupby(['datum'], group_keys=False).apply(adjusted_outlyingness, column = 'umsatz').reset_index(drop=True)\n",
    "df_school_shift = df_school_shift[df_school_shift['is_outlier'] == False]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43793a8",
   "metadata": {},
   "source": [
    "**Add UmsatzVJ fürs Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "60f5aa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_shift = df_no_shift.merge(df_no_shift[['datum', 'filialnummer', 'Bäckerei', 'umsatz']], left_on=['datumVJ', 'filialnummer', 'Bäckerei'], right_on=['datum', 'filialnummer', 'Bäckerei'], how='left', suffixes=('', 'VJ'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e3ed64cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_public_shift = df_public_shift.merge(df_public_shift[['datum', 'filialnummer', 'Bäckerei', 'umsatz']], left_on=['datumVJ', 'filialnummer', 'Bäckerei'], right_on=['datum', 'filialnummer', 'Bäckerei'], how='left', suffixes=('', 'VJ'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "846748eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_school_shift = df_school_shift.merge(df_school_shift[['datum', 'filialnummer', 'Bäckerei', 'umsatz']], left_on=['datumVJ', 'filialnummer', 'Bäckerei'], right_on=['datum', 'filialnummer', 'Bäckerei'], how='left', suffixes=('', 'VJ'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ff327d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_shift = df_no_shift.drop(['datumVJ'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7ea14c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_public_shift = df_public_shift.drop(['datumVJ'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dba05013",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_school_shift = df_school_shift.drop(['datumVJ'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec425b8a",
   "metadata": {},
   "source": [
    "**Add other dates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1b7f27c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for offset in range(1, 4):\n",
    "    \n",
    "    new_column_name = f\"datum-{offset}\"\n",
    "    df_no_shift[new_column_name] = df_no_shift[\"datum\"] - pd.DateOffset(weeks=offset)\n",
    "    \n",
    "for offset in range(1, 3):\n",
    "    \n",
    "    new_column_name = f\"datum+{offset}\"\n",
    "    df_no_shift[new_column_name] = df_no_shift[\"datum\"] + pd.DateOffset(weeks=offset)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b36f902d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for offset in range(1, 4):\n",
    "    \n",
    "    new_column_name = f\"datum-{offset}\"\n",
    "    df_public_shift[new_column_name] = df_public_shift[\"datum\"] - pd.DateOffset(weeks=offset)\n",
    "    \n",
    "for offset in range(1, 3):\n",
    "    \n",
    "    new_column_name = f\"datum+{offset}\"\n",
    "    df_public_shift[new_column_name] = df_public_shift[\"datum\"] + pd.DateOffset(weeks=offset)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "44237a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "for offset in range(1, 4):\n",
    "    \n",
    "    new_column_name = f\"datum-{offset}\"\n",
    "    df_school_shift[new_column_name] = df_school_shift[\"datum\"] - pd.DateOffset(weeks=offset)\n",
    "    \n",
    "for offset in range(1, 3):\n",
    "    \n",
    "    new_column_name = f\"datum+{offset}\"\n",
    "    df_school_shift[new_column_name] = df_school_shift[\"datum\"] + pd.DateOffset(weeks=offset)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835cd948",
   "metadata": {},
   "source": [
    "**Add umsatz**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a105e0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for offset in range(1, 4):\n",
    "    df_no_shift = df_no_shift.merge(df_no_shift[['datum', 'filialnummer', 'Bäckerei', 'umsatz']], left_on=[f\"datum-{offset}\", 'filialnummer', 'Bäckerei'], right_on=['datum', 'filialnummer', 'Bäckerei'], how='left', suffixes=('', f\"-{offset}\"))\n",
    "    df_no_shift = df_no_shift.loc[:,~df_no_shift.columns.duplicated()].copy()\n",
    "    df_no_shift = df_no_shift.merge(df_no_shift[['datum', 'filialnummer', 'Bäckerei', 'umsatzVJ']], left_on=[f\"datum-{offset}\", 'filialnummer', 'Bäckerei'], right_on=['datum', 'filialnummer', 'Bäckerei'], how='left', suffixes=('', f\"-{offset}\"))\n",
    "    df_no_shift = df_no_shift.loc[:,~df_no_shift.columns.duplicated()].copy()\n",
    "    \n",
    "for offset in range(1, 3):\n",
    "    df_no_shift = df_no_shift.merge(df_no_shift[['datum', 'filialnummer', 'Bäckerei', 'umsatz']], left_on=[f\"datum+{offset}\", 'filialnummer', 'Bäckerei'], right_on=['datum', 'filialnummer', 'Bäckerei'], how='left', suffixes=('', f\"+{offset}\"))\n",
    "    df_no_shift = df_no_shift.loc[:,~df_no_shift.columns.duplicated()].copy()\n",
    "    df_no_shift = df_no_shift.merge(df_no_shift[['datum', 'filialnummer', 'Bäckerei', 'umsatzVJ']], left_on=[f\"datum+{offset}\", 'filialnummer', 'Bäckerei'], right_on=['datum', 'filialnummer', 'Bäckerei'], how='left', suffixes=('', f\"+{offset}\"))\n",
    "    df_no_shift = df_no_shift.loc[:,~df_no_shift.columns.duplicated()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "82cd8d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for offset in range(1, 4):\n",
    "    df_public_shift = df_public_shift.merge(df_public_shift[['datum', 'filialnummer', 'Bäckerei', 'umsatz']], left_on=[f\"datum-{offset}\", 'filialnummer', 'Bäckerei'], right_on=['datum', 'filialnummer', 'Bäckerei'], how='left', suffixes=('', f\"-{offset}\"))\n",
    "    df_public_shift = df_public_shift.loc[:,~df_public_shift.columns.duplicated()].copy()\n",
    "    df_public_shift = df_public_shift.merge(df_public_shift[['datum', 'filialnummer', 'Bäckerei', 'umsatzVJ']], left_on=[f\"datum-{offset}\", 'filialnummer', 'Bäckerei'], right_on=['datum', 'filialnummer', 'Bäckerei'], how='left', suffixes=('', f\"-{offset}\"))\n",
    "    df_public_shift = df_public_shift.loc[:,~df_public_shift.columns.duplicated()].copy()\n",
    "    \n",
    "for offset in range(1, 3):\n",
    "    df_public_shift = df_public_shift.merge(df_public_shift[['datum', 'filialnummer', 'Bäckerei', 'umsatz']], left_on=[f\"datum+{offset}\", 'filialnummer', 'Bäckerei'], right_on=['datum', 'filialnummer', 'Bäckerei'], how='left', suffixes=('', f\"+{offset}\"))\n",
    "    df_public_shift = df_public_shift.loc[:,~df_public_shift.columns.duplicated()].copy()\n",
    "    df_public_shift = df_public_shift.merge(df_public_shift[['datum', 'filialnummer', 'Bäckerei', 'umsatzVJ']], left_on=[f\"datum+{offset}\", 'filialnummer', 'Bäckerei'], right_on=['datum', 'filialnummer', 'Bäckerei'], how='left', suffixes=('', f\"+{offset}\"))\n",
    "    df_public_shift = df_public_shift.loc[:,~df_public_shift.columns.duplicated()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d58c0940",
   "metadata": {},
   "outputs": [],
   "source": [
    "for offset in range(1, 4):\n",
    "    df_school_shift = df_school_shift.merge(df_school_shift[['datum', 'filialnummer', 'Bäckerei', 'umsatz']], left_on=[f\"datum-{offset}\", 'filialnummer', 'Bäckerei'], right_on=['datum', 'filialnummer', 'Bäckerei'], how='left', suffixes=('', f\"-{offset}\"))\n",
    "    df_school_shift = df_school_shift.loc[:,~df_school_shift.columns.duplicated()].copy()\n",
    "    df_school_shift = df_school_shift.merge(df_school_shift[['datum', 'filialnummer', 'Bäckerei', 'umsatzVJ']], left_on=[f\"datum-{offset}\", 'filialnummer', 'Bäckerei'], right_on=['datum', 'filialnummer', 'Bäckerei'], how='left', suffixes=('', f\"-{offset}\"))\n",
    "    df_school_shift = df_school_shift.loc[:,~df_school_shift.columns.duplicated()].copy()\n",
    "    \n",
    "for offset in range(1, 3):\n",
    "    df_school_shift = df_school_shift.merge(df_school_shift[['datum', 'filialnummer', 'Bäckerei', 'umsatz']], left_on=[f\"datum+{offset}\", 'filialnummer', 'Bäckerei'], right_on=['datum', 'filialnummer', 'Bäckerei'], how='left', suffixes=('', f\"+{offset}\"))\n",
    "    df_school_shift = df_school_shift.loc[:,~df_school_shift.columns.duplicated()].copy()\n",
    "    df_school_shift = df_school_shift.merge(df_school_shift[['datum', 'filialnummer', 'Bäckerei', 'umsatzVJ']], left_on=[f\"datum+{offset}\", 'filialnummer', 'Bäckerei'], right_on=['datum', 'filialnummer', 'Bäckerei'], how='left', suffixes=('', f\"+{offset}\"))\n",
    "    df_school_shift = df_school_shift.loc[:,~df_school_shift.columns.duplicated()].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a141b963",
   "metadata": {},
   "source": [
    "### **one hot encoded Variablen**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4e912a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_shift['datum+2'] = df_no_shift['datum+2'].astype(str)\n",
    "df_public_shift['datum+2'] = df_public_shift['datum+2'].astype(str)\n",
    "df_school_shift['datum+2'] = df_school_shift['datum+2'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "368d18b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beispiel-Listen mit Feiertagsdaten\n",
    "karneval = ['2023-02-13', '2024-02-05', '2025-02-24']\n",
    "ostern = ['2023-04-10', '2024-04-01', '2025-04-21']\n",
    "himmelfahrt = ['2023-05-15', '2024-05-06', '2024-05-26']\n",
    "pfingsten = ['2023-05-29', '2024-05-20', '2025-06-09']\n",
    "weihnachten = ['2022-12-26', '2023-12-25', '2024-12-23', '2025-12-25']\n",
    "neujahr = ['2023-02-01', '2024-01-01', '2025-03-01']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d2ab4ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_variables(row, liste):\n",
    "    if row['datum+2'] in liste:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "91206479",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_shift['karneval'] = df_no_shift.apply(lambda row: dummy_variables(row, karneval), axis=1)\n",
    "df_no_shift['ostern'] = df_no_shift.apply(lambda row: dummy_variables(row, ostern), axis=1)\n",
    "df_no_shift['himmelfahrt'] = df_no_shift.apply(lambda row: dummy_variables(row, himmelfahrt), axis=1)\n",
    "df_no_shift['pfingsten'] = df_no_shift.apply(lambda row: dummy_variables(row, pfingsten), axis=1)\n",
    "df_no_shift['weihnachten'] = df_no_shift.apply(lambda row: dummy_variables(row, weihnachten), axis=1)\n",
    "df_no_shift['neujahr'] = df_no_shift.apply(lambda row: dummy_variables(row, neujahr), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "38e104f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_public_shift['karneval'] = df_public_shift.apply(lambda row: dummy_variables(row, karneval), axis=1)\n",
    "df_public_shift['ostern'] = df_public_shift.apply(lambda row: dummy_variables(row, ostern), axis=1)\n",
    "df_public_shift['himmelfahrt'] = df_public_shift.apply(lambda row: dummy_variables(row, himmelfahrt), axis=1)\n",
    "df_public_shift['pfingsten'] = df_public_shift.apply(lambda row: dummy_variables(row, pfingsten), axis=1)\n",
    "df_public_shift['weihnachten'] = df_public_shift.apply(lambda row: dummy_variables(row, weihnachten), axis=1)\n",
    "df_public_shift['neujahr'] = df_public_shift.apply(lambda row: dummy_variables(row, neujahr), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "53971e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_school_shift['karneval'] = df_school_shift.apply(lambda row: dummy_variables(row, karneval), axis=1)\n",
    "df_school_shift['ostern'] = df_school_shift.apply(lambda row: dummy_variables(row, ostern), axis=1)\n",
    "df_school_shift['himmelfahrt'] = df_school_shift.apply(lambda row: dummy_variables(row, himmelfahrt), axis=1)\n",
    "df_school_shift['pfingsten'] = df_school_shift.apply(lambda row: dummy_variables(row, pfingsten), axis=1)\n",
    "df_school_shift['weihnachten'] = df_school_shift.apply(lambda row: dummy_variables(row, weihnachten), axis=1)\n",
    "df_school_shift['neujahr'] = df_school_shift.apply(lambda row: dummy_variables(row, neujahr), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32992b77",
   "metadata": {},
   "source": [
    "**Safe data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d5c3aa62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_shift = df_no_shift.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dbf1b20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_public_shift = df_public_shift.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0aee8df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_school_shift = df_school_shift.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9fe851",
   "metadata": {},
   "source": [
    "**Add unique column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f0fa99c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_shift['filiale_baeckerei'] = df_no_shift['filialnummer'].astype(str) + '_' + df_no_shift['Bäckerei']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9f26148f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_public_shift['filiale_baeckerei'] = df_public_shift['filialnummer'].astype(str) + '_' + df_public_shift['Bäckerei']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f48a2d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_school_shift['filiale_baeckerei'] = df_school_shift['filialnummer'].astype(str) + '_' + df_school_shift['Bäckerei']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a099101",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_shift = df_no_shift.rename(columns=rename_dict)\n",
    "df_public_shift = df_public_shift.rename(columns=rename_dict)\n",
    "df_school_shift = df_school_shift.rename(columns=rename_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "325d8b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_shift.to_csv('no_shift_forecasting.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "168a0ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_public_shift.to_csv('public_shift_forecasting.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e12ad42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_school_shift.to_csv('school_shift_forecasting.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
